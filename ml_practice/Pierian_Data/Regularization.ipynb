{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4f1c3b-eee5-45b1-86ab-2f6d566d171d",
   "metadata": {},
   "source": [
    "# Regularization Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f217cdf2-8843-4740-8242-82471fff8d9d",
   "metadata": {},
   "source": [
    "Regulation is the way to reduce model overfitting . It require some additional  bias  and search for optimal penalty hyperparameter .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d607171-afc6-4618-a7ea-3edaa5da9f08",
   "metadata": {},
   "source": [
    "### How it  is done\n",
    "- Minimizing model complexity\n",
    "- penalizing loss function\n",
    "- Reducing model-overfitting (by adding some bias to reduce variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea5975-6018-4c86-87af-37ac58dd9c13",
   "metadata": {},
   "source": [
    "# Types of Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e057b26-ba31-4a14-8128-6ce79b1b243e",
   "metadata": {},
   "source": [
    "1. <b>L1 regularization( or Lasso Regression)<b>\n",
    "2. <b>L2 regularization( or Ridge Regression)<b>\n",
    "3. <b> Elastic Net regularization( combinartion of l1 and l2)<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33b1b1-1963-47f3-a178-3926b0393424",
   "metadata": {},
   "source": [
    "# 1.L1 Regularization (Lasso Regression)\n",
    "- Adds a penalty equal to the absolute value of the magnitude of the coefficient to the loss function(i.e cost function)\n",
    "- It limits the size of the coefficient in the regression equation.\n",
    "- It can yield spare models where some coefficients  can be zero (In polynominal regression as we saw some coefficients were very small that are almost zero, it'll treat them as zero elimination the coefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b54cc-106d-4cb4-af8e-f089ba5f5604",
   "metadata": {},
   "source": [
    "$$ L1 = \\sum_{i=0}^{m-1}(y_i-\\hat y_i)^2 + \\lambda \\sum_{j=0}^{n-1}|\\beta_j| $$\n",
    "$$ which\\ is: $$\n",
    "$$ L1= SSR +\\lambda \\sum_{j=0}^{n-1}|\\beta_j|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393be87-1528-4d31-b32a-fc2a82ddfee4",
   "metadata": {},
   "source": [
    "Expanding Sum of Squared Residuals(SSR):\n",
    "$$ \\hat y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} +\\ ....\\ + \\beta_jx_{ij}$$\n",
    "$$ SSR = \\sum_{i=0}^{m-1} \\left( y_i - \\beta_0 - \\sum_{j=1}^{n}\\beta_j x_{ij}\\right)^2  $$\n",
    "- here SSR= Sum of Squared Residuals\n",
    "- $\\lambda $ is a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef113a2-2dec-4a37-b29d-c78bbaad626f",
   "metadata": {},
   "source": [
    "# 2.L2 Regularization (Ridge Regression)\n",
    "- Adds a penalty equal to the squared of the magnitude of coefficients.\n",
    "- All the coefficients are shrunk by same factor but doesnot necessarily eliminate them.\n",
    "    $$ L2 = \\sum_{i=0}^{m-1}(y_i - \\hat y_i)^2 + \\lambda \\sum_{j=0}^{n-1} (\\beta_j)^2$$\n",
    "  $$ which\\ is: $$\n",
    "  $$L2=SSR +\\lambda \\sum_{j=0}^{n-1}\\left(\\beta_j\\right)^2 $$\n",
    "\n",
    "- Theoretically, the ridge regression is:\n",
    "  $$ Error = SSR + Shrinkage Penalty $$\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47aaee45-3989-4a7b-8b04-fc15828e7b22",
   "metadata": {},
   "source": [
    "# 3. Elastic Net (Combination of L1 and L2)\n",
    "\n",
    "$$ Error = SSR + \\lambda \\sum_{j=0}^{n-1} \\beta_j^2 +\\lambda_2 \\sum_{j=0}^{n-1} |\\beta_j| $$\n",
    "- We can alternatively expres as ratio between L1 and L2\n",
    "  $$ Error = SSR + \\lambda \\left( \\frac{1-\\alpha}{2} \\sum_{j=0}^{n-1}\\beta^2+\\alpha\\sum_{j=0}^{n-1}|\\beta_j|\\right) $$\n",
    "\n",
    "  - here, $\\alpha$ and $\\lambda$ are the tunable parameters\n",
    "    - $\\alpha$ is used as the ratio between the L1 and L2\n",
    "    - when $\\alpha = 0$ we're only considering L2\n",
    "    - when $\\alpha = 1$ we're only considering L1\n",
    "    > Scikit learn's `ElasticNetCV` model represents $\\lambda$ as `alpha` parameter and `l1_ratio` for the $\\alpha$ we're representing here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b37f51-dfb7-486f-96ba-0e22111bab0e",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "- In regularization we are just adding some penalty term to the the error that we're trying to minimize\n",
    "- $\\lambda$ is the hyperparameter. $\\lambda = 0$ is same as not performing any regularization and becomes just a Sum of Squared Residuals (SSR)\n",
    "> Every tunable parameters are represented as $\\alpha$ in scikit learn. So donot confuse $\\lambda$ with $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695fc78-75c5-4a4c-878d-f712ac55ba06",
   "metadata": {},
   "source": [
    "# Revisit on Feature Scaling\n",
    "- Algorithim like gradient descent and KNN (which relys on distance metric) requires feature scaling to perform well\n",
    "- In gradient descent, the features with large scale will have their coefficient updated faster than the coefficient of small scaled features. Scaled features will allow gradient descent to converge efficiently.\n",
    "- There are some algorithms in ML where feature scaling will have no effect. (Regression trees, decision trees, random forest etc.)\n",
    "- Generally, decision tree based algorithms will have no effect with feature scaling\n",
    "> If we scale the training features, we'll have to scale the unseen data too before feeding it to the model\n",
    "\n",
    "- Improves coefficient interpreatability meaning we can relate and compare between coefficients and their impact.\n",
    "- It causes great increase in model performance\n",
    "\n",
    "> If we're not sure if we need to scale or not, we can scale it anyway. It has no drawback and doesnot affect the data set in any conditions. Only thing to remember is to scale the unseen data (with the same scaling factors as in trained model) before feeding it to the model\n",
    "\n",
    "#### Ways to scale features\n",
    "1. <b>Standardization</b>: Rescale data to have mean $(\\mu) = 0$ and standard deviation $(\\sigma) = 1$. It is also known as Z-score normalization\n",
    "      $$x_{1,scaled} = \\frac{x_1 - \\mu_1}{\\sigma_1}$$\n",
    "2. <b>Normalization</b>: Rescale all the data values to be between $0\\ and\\ 1$\n",
    "      $$x_{1,scaled} = \\frac{x_1 - x_{min}}{x_{max} - x_{min}}$$\n",
    "> While performing featuring scaling mean, standard deviation, min and max should be calculated for train data. Using all the data set(both train and test) will cause information leakage to the training data. We can use .fit() and .transform() method in scikit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2406de94-6a62-44d1-8601-a23fb343ad7d",
   "metadata": {},
   "source": [
    "# Cross-Validation Overview\n",
    "- It is way to estimate errors wihout having to split data set into train and test sets\n",
    "- As we're not splitting dataset for test, we'll have advantage of more data points to train our model(generally 30% of the data set would have splitted for model test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31c8f1-7ccb-4d07-aca4-a4de34f3cee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75fcc1-950c-4b52-8788-00e5cb9a9648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
