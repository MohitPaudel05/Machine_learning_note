{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4f1c3b-eee5-45b1-86ab-2f6d566d171d",
   "metadata": {},
   "source": [
    "# Regularization Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f217cdf2-8843-4740-8242-82471fff8d9d",
   "metadata": {},
   "source": [
    "Regulation is the way to reduce model overfitting . It require some additional  bias  and search for optimal penalty hyperparameter .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d607171-afc6-4618-a7ea-3edaa5da9f08",
   "metadata": {},
   "source": [
    "### How it  is done\n",
    "- Minimizing model complexity\n",
    "- penalizing loss function\n",
    "- Reducing model-overfitting (by adding some bias to reduce variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea5975-6018-4c86-87af-37ac58dd9c13",
   "metadata": {},
   "source": [
    "# Types of Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e057b26-ba31-4a14-8128-6ce79b1b243e",
   "metadata": {},
   "source": [
    "1. <b>L1 regularization( or Lasso Regression)<b>\n",
    "2. <b>L2 regularization( or Ridge Regression)<b>\n",
    "3. <b> Elastic Net regularization( combinarion of l1 and l2)<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33b1b1-1963-47f3-a178-3926b0393424",
   "metadata": {},
   "source": [
    "# 1.L1 Regularization (Lasso Regression)\n",
    "- Adds a penalty equal to the absolute value of the magnitude of the coefficient to the loss function(i.e cost function)\n",
    "- It limits the size of the coefficient in the regression equation.\n",
    "- It can yield spare models where some coefficients  can be zero (In polynominal regression as we saw some coefficients were very small that are almost zero, it'll treat them as zero elimination the coefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b54cc-106d-4cb4-af8e-f089ba5f5604",
   "metadata": {},
   "source": [
    "$$ L1 = \\sum_{i=0}^{m-1}(y_i-\\hat y_i)^2 + \\lambda \\sum_{j=0}^{n-1}|\\beta_j| $$\n",
    "$$ which\\ is: $$\n",
    "$$ L1= SSR +\\lambda \\sum_{j=0}^{n-1}|\\beta_j|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393be87-1528-4d31-b32a-fc2a82ddfee4",
   "metadata": {},
   "source": [
    "Expanding Sum of Squared Residuals(SSR):\n",
    "$$ \\hat y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} +\\ ....\\ + \\beta_jx_{ij}$$\n",
    "$$ SSR = \\sum_{i=0}^{m-1} \\left( y_i - \\beta_0 - \\sum_{j=1}^{n}\\beta_j x_{ij}\\right)^2  $$\n",
    "- here SSR= Sum of Squared Residuals\n",
    "- $\\lambda $ is a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef113a2-2dec-4a37-b29d-c78bbaad626f",
   "metadata": {},
   "source": [
    "# 2.L2 Regularization (Ridge Regression)\n",
    "- Adds a penalty equal to the squared of the magnitude of coefficients.\n",
    "- All the coefficients are shrunk by same factor but doesnot necessarily eliminate them.\n",
    "    $$ L2 = \\sum_{i=0}^{m-1}(y_i - \\hat y_i)^2 + \\lambda \\sum_{j=0}^{n-1} (\\beta_j)^2$$\n",
    "  $$ which\\ is: $$\n",
    "  $$L2=SSR +\\lambda \\sum_{j=0}^{n-1}\\left(\\beta_j\\right)^2 $$\n",
    "\n",
    "- Theoretically, the ridge regression is:\n",
    "  $$ Errro = SSR + \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db462cc-b607-413f-bbef-38cea5216efd",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e855e-5508-4b66-a4fb-9ecdb0dacb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
